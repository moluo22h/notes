# 模型格式转换

## 模型格式

机器学习模型导出的格式多种多样，不同的格式适用于不同的场景和框架，以下是一些常见的格式：

### 深度学习框架原生格式
- **PyTorch的.pth和.pt**
    - **说明**：PyTorch中常用的模型保存格式。`.pth`和`.pt`本质上是一样的，通常用于保存模型的参数、优化器状态等信息。可以通过`torch.save()`函数将模型的状态字典（state_dict）保存为这种格式，加载时使用`torch.load()`函数。
    - **应用场景**：在PyTorch项目内部进行模型的保存和加载，方便模型的训练中断后继续训练、模型评估以及模型的迁移等操作。
- **TensorFlow的.ckpt和SavedModel**
    - **.ckpt**：TensorFlow早期版本中用于保存模型参数的格式，通常会有一个检查点文件（checkpoint file）和多个数据文件，分别保存模型的权重、偏置等参数以及相关的元数据。
    - **SavedModel**：TensorFlow 2.x推荐的模型保存格式，它是一种更完整、更通用的模型保存方式，不仅包含模型的参数，还包含模型的计算图结构、变量信息等，便于在不同的环境中部署和使用。
    - **应用场景**：`.ckpt`格式常用于TensorFlow 1.x版本的模型训练和恢复。SavedModel则适用于TensorFlow 2.x的模型部署，包括在TensorFlow Serving等服务框架中进行模型的部署和推理。

### 通用的模型格式
- **ONNX**
    - **说明**：开放神经网络交换（Open Neural Network Exchange）格式，是一种用于表示深度学习模型的标准格式，旨在实现不同深度学习框架之间的模型转换和互操作性。它定义了一组统一的模型表示规范，包括模型的结构、参数、数据类型等。
    - **应用场景**：当需要在不同的深度学习框架之间共享模型，或者将模型部署到不支持特定框架原生格式的环境中时，ONNX格式非常有用。例如，将PyTorch训练的模型转换为ONNX格式后，可以在TensorFlow或其他支持ONNX的推理引擎中进行推理。
- **TensorRT的.engine**
    - **说明**：TensorRT是NVIDIA推出的用于高性能深度学习推理的SDK，`.engine`是TensorRT优化后的模型格式。它针对NVIDIA的GPU硬件进行了高度优化，通过一系列的优化技术，如层融合、量化等，提高模型的推理速度和效率。
    - **应用场景**：在NVIDIA的GPU设备上进行深度学习模型的高性能推理部署，特别是在对实时性要求较高的场景，如自动驾驶、视频监控等领域。

### 其他格式
- **H5**
    - **说明**：HDF5（Hierarchical Data Format 5）的一种文件扩展名，常用于保存Keras模型。它可以保存模型的结构、权重以及训练配置等所有信息，使得模型可以方便地被加载和继续训练或进行推理。
    - **应用场景**：在Keras框架中进行模型的保存和加载，尤其是在将模型用于不同的Keras项目或与其他支持H5格式的工具进行交互时。
- **PMML**
    - **说明**：预测模型标记语言（Predictive Model Markup Language），是一种基于XML的格式，用于表示数据挖掘和机器学习模型。它定义了一套标准的模式和规则，用于描述模型的结构、算法、参数等信息，使得不同的数据分析和机器学习工具之间能够共享和交换模型。
    - **应用场景**：在数据挖掘和机器学习的企业级应用中，当需要在不同的软件平台和工具之间共享模型，或者进行模型的审计、监管等操作时，PMML格式非常有用。

## 格式转换

不同机器学习模型格式之间的相互转换可以满足在不同框架、平台或场景下使用模型的需求，以下是一些常见模型格式之间相互转换的方法和工具：

### 安装依赖库

在运行上述代码前，你需要安装相应的依赖库，例如：

```bash
pip install torch torchvision onnx onnx2pytorch tensorflow tf2onnx
```

同时，对于 TensorRT 相关的转换，你需要正确安装 TensorRT 并配置好环境。不同的环境和版本可能会有细微的差异，在实际使用时需要根据具体情况进行调整。

### PyTorch与ONNX的转换
- **PyTorch转ONNX**：PyTorch提供了`torch.onnx.export()`函数来实现转换。首先加载训练好的PyTorch模型，然后定义一个输入张量，指定输出的ONNX模型文件名和输入输出等参数，即可将PyTorch模型转换为ONNX格式。

  ```python
  import torch
  import torchvision.models as models
  
  # 加载预训练的 PyTorch 模型
  model = models.resnet18(pretrained=True)
  model.eval()
  
  # 定义输入张量
  dummy_input = torch.randn(1, 3, 224, 224)
  
  # 导出为 ONNX 格式
  torch.onnx.export(model, dummy_input, "resnet18.onnx",
                    export_params=True,
                    opset_version=11,
                    do_constant_folding=True,
                    input_names=['input'],
                    output_names=['output'],
                    dynamic_axes={'input': {0: 'batch_size'},
                                  'output': {0: 'batch_size'}})
  ```

  在上述代码中，首先加载了预训练的 ResNet - 18 模型，然后创建了一个虚拟输入张量，最后使用 `torch.onnx.export` 函数将模型导出为 ONNX 格式。

- **ONNX转PyTorch**：可以使用`onnx2pytorch`库。安装该库后，通过调用相关函数，将ONNX模型加载进来，即可转换为PyTorch模型。不过由于ONNX到PyTorch的转换可能存在一些不支持的操作或特性，可能需要对转换后的模型进行一些调整和验证。

  ```python
  from onnx2pytorch import ConvertModel
  import onnx
  
  # 加载 ONNX 模型
  onnx_model = onnx.load('resnet18.onnx')
  
  # 转换为 PyTorch 模型
  pytorch_model = ConvertModel(onnx_model)
  ```

  这里使用 `onnx2pytorch` 库将 ONNX 模型转换为 PyTorch 模型。

### TensorFlow与ONNX的转换
- **TensorFlow转ONNX**：可以使用`tf2onnx`工具。对于TensorFlow 1.x模型，先将模型转换为SavedModel格式，然后使用`tf2onnx.convert()`函数进行转换。对于TensorFlow 2.x模型，可以直接将Keras模型或其他TF 2.x模型转换为ONNX格式，转换时需要指定模型的输入输出签名等信息。

  ```python
  import tensorflow as tf
  import tf2onnx
  import onnx
  
  # 构建并训练一个简单的 Keras 模型
  model = tf.keras.Sequential([
      tf.keras.layers.Dense(64, activation='relu', input_shape=(784,)),
      tf.keras.layers.Dense(10, activation='softmax')
  ])
  model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])
  
  # 转换为 ONNX 格式
  input_signature = [tf.TensorSpec([None, 784], tf.float32, name='input')]
  output_path = "model.onnx"
  model_proto, _ = tf2onnx.convert.from_keras(model, input_signature, opset=13)
  onnx.save(model_proto, output_path)
  ```

  此代码先构建并编译了一个简单的 Keras 模型，然后使用 `tf2onnx.convert.from_keras` 函数将其转换为 ONNX 格式。

- **ONNX转TensorFlow**：可以使用`onnx-tensorflow`库。安装后，通过调用相关函数加载ONNX模型并进行转换。但同样可能存在一些兼容性问题，需要对转换后的模型进行测试和调整。

### TensorFlow与TensorRT的转换
- **TensorFlow转TensorRT**：在NVIDIA的TensorRT环境中，可以使用`tf2trt`工具。首先将TensorFlow模型转换为冻结的PB（Protocol Buffer）格式，然后使用`tf2trt`的相关函数将PB模型转换为TensorRT的`.engine`格式，转换过程中可以进行一些优化配置，如指定精度、批处理大小等。

  ```python
  import tensorflow as tf
  import tensorflow.contrib.tensorrt as trt
  
  # 加载 TensorFlow 模型
  saved_model_dir = 'path/to/saved_model'
  output_saved_model_dir = 'path/to/trt_saved_model'
  
  # 构建 TensorRT 优化模型
  trt_graph = trt.create_inference_graph(
      input_graph_def=None,
      outputs=None,
      max_batch_size=1,
      max_workspace_size_bytes=1 << 25,
      precision_mode='FP16',
      minimum_segment_size=3,
      is_dynamic_op=True,
      input_saved_model_dir=saved_model_dir,
      input_saved_model_tags=[tf.saved_model.SERVING]
  )
  
  # 保存 TensorRT 优化后的模型
  tf.io.write_graph(trt_graph, output_saved_model_dir, 'trt_graph.pb', as_text=False)
  ```

  该代码将 TensorFlow 的 SavedModel 转换为 TensorRT 优化的图，并保存为 `pb` 文件。需要注意的是，上述代码在 TensorFlow 1.x 环境下运行，TensorFlow 2.x 中对 TensorRT 的集成方式有所不同。

- **TensorRT转TensorFlow**：TensorRT到TensorFlow的直接转换相对复杂且不是常见需求。一般可以通过分析TensorRT模型的结构和参数，在TensorFlow中重新构建类似的模型，并将TensorRT模型的参数加载到TensorFlow模型中，但这需要对两个框架的底层原理有深入理解，且可能需要大量的手动操作。

### PyTorch与TensorFlow的转换
- **PyTorch转TensorFlow**：可以先将PyTorch模型转换为ONNX格式，再从ONNX转换为TensorFlow格式。也有一些直接转换的方法，如使用`pytorch2keras`库将PyTorch模型转换为Keras模型，再将Keras模型转换为TensorFlow模型，但可能需要对模型结构和数据类型进行一些调整。
- **TensorFlow转PyTorch**：类似地，可以通过ONNX作为中间格式进行转换。也可以手动分析TensorFlow模型的结构和参数，在PyTorch中重新构建模型并加载参数，但这种方法较为繁琐，且需要对两个框架有深入了解。

### 其他转换
- **H5与其他格式转换**：Keras的H5模型与其他格式的转换可以借助相应框架的功能。例如，Keras模型可以通过`tf.keras.models.save_model()`函数保存为H5格式，也可以从H5格式加载模型。要将H5模型转换为其他格式，如ONNX，可以先加载H5模型，然后使用相应的转换工具将其转换为目标格式。
- **PMML与其他格式转换**：许多数据挖掘和机器学习工具都支持PMML格式的导入和导出。例如，在Python中，可以使用`pypmml`库来处理PMML模型。要将其他格式的模型转换为PMML，可以使用支持PMML输出的机器学习框架或工具，在训练完模型后，将其导出为PMML格式。反之，要将PMML模型转换为其他格式，可以先使用`pypmml`等库加载PMML模型，然后根据目标格式的要求，在相应的框架中重新构建模型并加载PMML模型中的参数和配置信息。

## 模型部署

不同类型的机器学习模型部署方式有所不同，下面分别介绍常见的深度学习模型（如基于 PyTorch、TensorFlow）、传统机器学习模型（如 Scikit - learn 模型）的部署方法。

### 深度学习模型部署

#### 1. 基于 PyTorch 的模型部署
- **本地部署**
    - **步骤**：
        - 加载模型：使用 `torch.load` 加载保存好的模型参数。
        - 预处理输入数据：根据模型训练时的要求对输入数据进行预处理，如归一化、调整尺寸等。
        - 进行推理：将预处理后的输入数据传入模型，调用 `model.eval()` 进入评估模式，然后进行前向传播得到预测结果。
    - **示例代码**：
```python
import torch
import torchvision.models as models
import torchvision.transforms as transforms
from PIL import Image

# 加载预训练的 ResNet 模型
model = models.resnet18(pretrained=True)
model.eval()

# 定义图像预处理转换
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# 加载并预处理图像
image = Image.open('test_image.jpg')
input_tensor = preprocess(image)
input_batch = input_tensor.unsqueeze(0)

# 进行推理
with torch.no_grad():
    output = model(input_batch)
```
- **使用 ONNX 进行跨平台部署**
    - **步骤**：
        - 将 PyTorch 模型转换为 ONNX 格式，使用 `torch.onnx.export` 函数。
        - 选择合适的推理引擎，如 ONNX Runtime。
        - 加载 ONNX 模型并进行推理。
    - **示例代码**：
```python
import onnxruntime as ort
import numpy as np
import torchvision.transforms as transforms
from PIL import Image

# 加载并预处理图像
preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])
image = Image.open('test_image.jpg')
input_tensor = preprocess(image)
input_batch = input_tensor.unsqueeze(0).numpy()

# 加载 ONNX 模型
ort_session = ort.InferenceSession("resnet18.onnx")

# 进行推理
input_name = ort_session.get_inputs()[0].name
outputs = ort_session.run(None, {input_name: input_batch})
```

#### 2. 基于 TensorFlow 的模型部署
- **本地部署**
    - **步骤**：
        - 加载模型：使用 `tf.keras.models.load_model` 加载保存好的 Keras 模型。
        - 预处理输入数据：按照模型训练时的要求对输入数据进行处理。
        - 进行推理：调用模型的 `predict` 方法得到预测结果。
    - **示例代码**：
```python
import tensorflow as tf
import numpy as np

# 加载模型
model = tf.keras.models.load_model('my_model.h5')

# 生成随机输入数据
input_data = np.random.rand(1, 784)

# 进行推理
predictions = model.predict(input_data)
```
- **使用 TensorFlow Serving 进行服务化部署**
    - **步骤**：
        - 将模型保存为 SavedModel 格式，使用 `tf.keras.models.save_model`。
        - 安装并启动 TensorFlow Serving。
        - 发送请求进行推理。
    - **示例代码（Python 客户端）**：
```python
import requests
import numpy as np

# 生成随机输入数据
input_data = np.random.rand(1, 784).tolist()

# 定义请求数据
data = {
    "instances": input_data
}

# 发送 POST 请求
response = requests.post('http://localhost:8501/v1/models/my_model:predict', json=data)

# 解析响应
predictions = response.json()['predictions']
```

### 传统机器学习模型部署（以 Scikit - learn 为例）
- **本地部署**
    - **步骤**：
        - 加载模型：使用 `joblib.load` 或 `pickle.load` 加载保存好的模型。
        - 预处理输入数据：按照训练时的方式对输入数据进行处理。
        - 进行推理：调用模型的 `predict` 方法得到预测结果。
    - **示例代码**：
```python
import joblib
import numpy as np

# 加载模型
model = joblib.load('my_model.joblib')

# 生成随机输入数据
input_data = np.random.rand(1, 10)

# 进行推理
predictions = model.predict(input_data)
```
- **Web 服务部署**
    - 可以使用 Flask 或 FastAPI 等 Web 框架将模型封装成 API 服务。
    - **示例代码（Flask）**：
```python
from flask import Flask, request, jsonify
import joblib
import numpy as np

app = Flask(__name__)

# 加载模型
model = joblib.load('my_model.joblib')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.get_json(force=True)
    input_data = np.array(data['input'])
    predictions = model.predict(input_data)
    return jsonify({'predictions': predictions.tolist()})

if __name__ == '__main__':
    app.run(debug=True)
```

以上是不同类型模型常见的部署方式，实际部署时还需要考虑性能优化、安全性、可扩展性等因素。 