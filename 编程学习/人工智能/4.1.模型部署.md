# 各类推理框架



不同的训练框架，基本都有对应的推理框架。cube为tf/torch/onnx/tensorrt/lightgbm/paddle/sklearn/xgboost等模型提供推理服务，基本仅需要用户填写模型名称和版本、模型地址，会自动生成所有的配置文件，提供api的demo，自动提供域名和ip的访问。

各类型的推理框架或多或少可以代理其他训练框架的模型进行推理服务。例如tfserving也可以为onnx模型提供推理服务，所以这些框架和模型并不是唯一绑定关系。

## tfserving



tfserving主要是tf模型推理服务，虽然同时也可以为其他模型提供推理服务的，比如上面说的onnx模型也是可以用tfserving提供推理服务的。

tfserving推理主要需要提供models.config、monitoring.config、platform.config等配置文件。推理服务提供了Model status API、Model Metadata API、Classify and Regress API、Predict API等类型的接口。

可以使用Model Metadata API作为模型的健康检查接口。

monitoring.config中配置的metric可以用于prometheus监控。

## torchserver



torch保存模型结构和参数完成信息后，需要先使用torch-model-archiver将模型压缩为可直接推理的包。主要是将http接口封装进去。

```
torch-model-archiver --model-name $model_name --version $model_version --handler image_classifier --serialized-file $model_version/$model_name --export-path $model_version -f
```



其中 --handler 支持如下 image_classifier，image_segmenter，object_detector，text_classifier 或自定义py函数。这个handler就是接口处理方式，也就决定了用户客户端端该如何请求。

torch server主要配置为config.properties和log4j.properties，提供的api包括推理（8080端口）、管理（8081）、监控（8082）

8080/ping可以用于pod的健康检查，POST /metrics可以用于prometheus的监控

## onnxruntime



pass

# 模型压缩



pass



## 参考文档

[如何基于MLServer构建Python机器学习服务-CSDN博客](https://blog.csdn.net/u010665216/article/details/129231299)

[Inference Runtimes — MLServer Documentation](https://mlserver.readthedocs.io/en/latest/runtimes/index.html#included-inference-runtimes)