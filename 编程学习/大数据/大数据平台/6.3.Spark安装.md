# Spark安装

## 安装

1. 下载spark安装包并解压，下载地址：[Downloads | Apache Spark](https://spark.apache.org/downloads.html)

   ```bash
   wget https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-without-hadoop.tgz
   tar -xvf spark-3.5.5-bin-without-hadoop.tgz -C /opt
   ```

   > 扩展：自带hadoop的版本使用如下命令：wget https://dlcdn.apache.org/spark/spark-3.5.5/spark-3.5.5-bin-hadoop3.tgz

2. 使用`vi /etc/profile`命令配置环境变量。添加以下变量

   ```bash
   export SPARK_HOME=/opt/spark-3.5.5-bin-without-hadoop
   export PATH=$SPARK_HOME/bin:$PATH
   ```

   保存文件后，运行以下命令使环境变量生效：

   ```bash
   source /etc/profile
   ```

3. 配置hadoop

   ```bash
   cd $SPARK_HOME/conf
   cp spark-env.sh.template spark-env.sh
   vi spark-env.sh
   ```

   ```bash
   export SPARK_DIST_CLASSPATH=$(/opt/hadoop-3.4.1/bin/hadoop classpath)
   ```

   > 注意：`/opt/hadoop-3.4.1/`是小编的hadoop安装位置，请根据实际环境自行更改

4. 通过`spark-shell`命令验证是否安装成功

   ```bash
   # spark-shell
   Spark context Web UI available at http://10.0.2.15:4040
   Spark context available as 'sc' (master = local[*], app id = local-1742877078129).
   Spark session available as 'spark'.
   Welcome to
         ____              __
        / __/__  ___ _____/ /__
       _\ \/ _ \/ _ `/ __/  '_/
      /___/ .__/\_,_/_/ /_/\_\   version 3.5.5
         /_/
            
   Using Scala version 2.12.18 (OpenJDK 64-Bit Server VM, Java 1.8.0_44)
   Type in expressions to have them evaluated.
   Type :help for more information.
   
   scala> 
   ```



## 配置

Spark支持配置以下模式，请根据实际需要选择

- Local模式：单机模式
- 集群模式：
  - Standalone模式：使用Spark自带的简单集群管理器
  - YARN模式：使用YARN作为集群管理器
  - Mesos模式：使用Mesos作为集群管理器
  - Kubernetes模式：实验阶段

### local模式

Spark安装完成，便可使用Spark的local模式了。

执行`./bin/spark-shell`即可进入scala交互界面：

```bash
scala> var r = sc.parallelize(Array(1,2,3,4))
r: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:23

scala> r.map(_*10).collect()
res0: Array[Int] = Array(10, 20, 30, 40)                                        
```

> 扩展：使用如下命令可以读取文件
>
> ```bash
> val file = sc.textFile("file:/home/hadoop/test.csv")
> file.first()
> file.count()
> ```

### standalone模式

1. 使用`vi spark.env.sh`编辑配置，添加以下内容：

   ```bash
   export JAVA_HOME=/opt/jdk1.8.0_191
   export HADOOP_CONF_DIR=/opt/hadoop-3.4.1/etc/hadoop
   ```

   > 提示：若workers配置文件不存在，可通过模板复制
   >
   > ```bash
   > cp spark-env.sh.template spark-env.sh
   > ```

2. 使用`vi workers`编辑配置，添加工作节点地址。默认已配置`localhost`，无特殊要求不用更改

   ```bash
   localhost
   ```

   > 提示：若workers配置文件不存在，可通过模板复制
   >
   > ```bash
   > cp workers.template workers
   > ```
   >
   > 注意：老版本spark，如2.4.4使用：cp slaves.template slaves

3. 启动服务

   ```bash
   sbin/start-all.sh
   ```

   > 提示：
   >
   > 可以使用`start-master.sh`、`start-slave.sh`和`start-slaves.sh`分别启动Master节点，本机slave节点和所有slave节点
   >
   > 可以使用`stop-master.sh`、`stop-slave.sh`和`stop-slaves.sh`分别停止Master节点，本机slave节点和所有slave节点

4. 查看进程

   ```bash
   # jps
   10062 Master
   10190 Worker
   ```

5. 浏览器访问http://192.168.10.50:8080

   ![image-20250327103502945](D:\user\person\notes\编程学习\大数据\大数据平台\assets\image-20250327103502945.png)

6. 执行`./spark-shell`指定`--master`为`spark`即可进入standalone模式

   ```bash
   ./spark-shell --master spark://192.168.10.50:7077
   ```

   > 提示：也可以使用其他客户端，如pyspark客户端：pyspark --master spark://192.168.10.50:7077

### on yarn模式

On Yarn模式和Standalone模式不同之处在于，OnYarn模式使用Hadoop中的Yarn作为资源管理器，可以使spark程序跑在Hadoop集群当中，Spark on Yarn又分为yarn-cluster和yarn-client。

Yarn Cluster:主程序逻辑和任务都运行在Yarn集群中

Yarn Client:主程序逻辑运行在本地，任务运行在Yarn集群中

1. 使用`vi spark.env.sh`编辑配置，添加以下内容

   ```bash
   # 添加JAVA_HOME
   export JAVA_HOME=/opt/jdk1.8.0_191
   
   # 添加HADOOP配置目录和YARN配置目录
   export HADOOP_CONF_DIR=/opt/hadoop-3.4.1/etc/hadoop
   export YARN_CONF_DIR=/opt/hadoop-3.4.1/etc/hadoop
   ```

   > 提示：若workers配置文件不存在，可通过模板复制
   >
   > ```bash
   > cp spark-env.sh.template spark-env.sh
   > ```

3. 执行`./spark-shell`指定`--master`为`yarn`即可进入standalone模式

   ```bash
   ./spark-shell --master yarn --name testapp
   ```



## 常见问题

### 问题一

```bash
# spark-shell
Error: A JNI error has occurred, please check your installation and try again
Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/hadoop/fs/FSDataInputStream
        at java.lang.Class.getDeclaredMethods0(Native Method)
        at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
        at java.lang.Class.privateGetMethodRecursive(Class.java:3048)
        at java.lang.Class.getMethod0(Class.java:3018)
        at java.lang.Class.getMethod(Class.java:1784)
        at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)
        at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.fs.FSDataInputStream
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 7 more
```

原因：缺失hadoop相关的配置

解决方式：

进入spark配置目录，创建对应的配置文件

```bash
cd $SPARK_HOME/conf
cp spark-env.sh.template spark-env.sh
```

使用`vi spark-env.sh`编辑配置文件，添加以下hadoop配置

```bash
export SPARK_DIST_CLASSPATH=$(/opt/hadoop-3.4.1/bin/hadoop classpath)
```

> 注意：`/opt/hadoop-3.4.1/`是小编的hadoop安装位置，请根据实际环境自行更改

### 问题二

```bash
25/04/01 17:06:13 WARN StandaloneAppClient$ClientEndpoint: Failed to connect to master 192.168.10.50:7077
org.apache.spark.SparkException: Exception thrown in awaitResult: 
        at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)
        at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)
        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
        at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)
        at org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)
        at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anon$1.run(StandaloneAppClient.scala:108)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
```

解决方式：

修改了Master的启动方法：添加master启动参数

```bash
./sbin/start-master.sh -h 192.168.10.50
```

成功启动后，通过如下命令，启动了Slave

```bash
./sbin/start-slave.sh spark://192.168.10.50:7077
```



## 参考文档

[Spark环境搭建（保姆级教程）-CSDN博客](https://blog.csdn.net/tangyi2008/article/details/123109198)