# Spark安装

## local模式

1. 下载spark安装包并解压

2. 执行`./bin/spark-shell`即可进入local模式

   ```bash
   val file = sc.textFile("file:/home/hadoop/test.csv")
   file.first()
   file.count()
   ```

## standalone模式

1. 下载spark安装包并解压

2. 编辑配置，配置路径位于/conf/目录下

   ```bash
   cp spark.env.sh.template spark.env.sh
   
   # 添加JAVA_HOME
   export JAVA_HOME=/soft/home/jdk1.8.0_191
   
   # 添加Spark配置目录
   export SPARK_CONF_DIR=/soft/home/spark-2.4.4-bin-hadoop2.7/conf
   ```

   ```bash
   cp slaves.template slaves
   ```

3. 执行

   ```bash
   ./sbin/start-master.sh
   ./sbin/start-slaves.sh
   ```

4. 浏览器访问http://{ip}:8080

5. 执行`./spark-shell`指定`--master`为`spark`即可进入standalone模式

   ```bash
   ./spark-shell --master spark://imooc:7077
   ```

## on yarn模式

On Yarn模式和Standalone模式不同之处在于，OnYarn模式使用Hadoop中的Yarn作为资源管理器，可以使spark程序跑在Hadoop集群当中，Spark on Yarn又分为yarn-cluster和yarn-client。

Yarn Cluster:主程序逻辑和任务都运行在Yarn集群中

Yarn Client:主程序逻辑运行在本地，任务运行在Yarn集群中



1. 下载spark安装包并解压

2. 编辑配置，配置路径位于/conf/目录下

   ```bash
   cp spark.env.sh.template spark.env.sh
   
   # 添加JAVA_HOME
   export JAVA_HOME=/soft/home/jdk1.8.0_191
   
   # 添加Spark配置目录
   export SPARK_CONF_DIR=/soft/home/spark-2.4.4-bin-hadoop2.7/conf
   
   # 添加HADOOP配置目录和YARN配置目录
   export HADOOP_CONF_DIR=/soft/home/hadoop-2.8.5/etc/hadoop
   export YARN_CONF_DIR=/soft/home/hadoop-2.8.5/etc/hadoop
   ```

3. 执行`./spark-shell`指定`--master`为`yarn`即可进入standalone模式

   ```bash
   ./spark-shell --master yarn --name testapp
   ```