# Hadoop环境搭建指南

## 介绍

Hadoop是一个用于处理大规模数据集的分布式计算框架，具有高容错性、可伸缩性和可靠性。本文将向您介绍如何搭建Hadoop环境。

## 环境准备

在开始之前，请确保您已按照以下步骤准备好环境：

1. 安装Java开发工具包（JDK），推荐使用Java 8或更新版本。
2. 下载Hadoop发行版，可从[官方网站](https://hadoop.apache.org/release.html)获取最新版本。其他下载地址：[Index of /hadoop/common (apache.org)](https://downloads.apache.org/hadoop/common/)
3. 准备一台或多台运行Linux操作系统的计算机。

## 步骤

### 1. 解压Hadoop发行版

将下载的Hadoop压缩包解压到您选择的目录下：

```shell
tar -zxvf hadoop-x.x.x.tar.gz
```

### 2. 配置环境变量

编辑`~/.bashrc`文件（如果使用其他shell，请相应地编辑对应的配置文件），并添加以下变量：

```shell
export HADOOP_HOME=/path/to/your/hadoop/directory
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

保存文件后，运行以下命令使环境变量生效：

```shell
source ~/.bashrc
```

### 3. 配置Hadoop

进入Hadoop的配置目录：

```shell
cd $HADOOP_HOME/etc/hadoop
```

#### 3.1 配置Hadoop集群

编辑`hadoop-env.sh`文件，将`JAVA_HOME`设置为您的JDK安装路径：

```shell
export JAVA_HOME=/path/to/your/jdk/directory
```

#### 3.2 配置HDFS

编辑`core-site.xml`文件，添加以下配置：

```xml
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://localhost:9000</value>
</property>
```

编辑`hdfs-site.xml`文件，添加以下配置：

```xml
<property>
  <name>dfs.replication</name>
  <value>1</value>
</property>

<property>
  <name>dfs.namenode.name.dir</name>
  <value>file:///path/to/your/hadoop/data/dfs/name</value>
</property>

<property>
  <name>dfs.datanode.data.dir</name>
  <value>file:///path/to/your/hadoop/data/dfs/data</value>
</property>
```

#### 3.3 配置YARN

编辑`yarn-site.xml`文件，添加以下配置：

```xml
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>
```

### 4. 格式化HDFS

运行以下命令初始化HDFS的元数据：

```shell
hdfs namenode -format
```

### 5. 启动Hadoop集群

运行以下命令启动Hadoop集群：

```bash
start-dfs.sh
start-yarn.sh
```

### 6. 验证安装

在浏览器中访问`http://localhost:9870`，您将看到Hadoop集群的Web界面。检查各个组件的状态以确保一切正常运行。

在浏览器中访问`http://localhost:8088`，您将看到Yarn的Web界面。检查各个组件的状态以确保一切正常运行。

## 结论

至此，您已成功搭建了Hadoop环境。您可以开始使用Hadoop来处理和分析大规模数据集了。祝您使用愉快！

> 注意：本文仅提供了基本的Hadoop环境搭建指南，具体的配置和调优取决于您的需求和环境。建议参考Hadoop官方文档和其他资源获取更多详细信息。

## 附录

hadoop 的启动和停止命令

```
sbin/start-all.sh 启动所有的Hadoop守护进程。包括NameNode、 Secondary NameNode、DataNode、ResourceManager、NodeManager
sbin/stop-all.sh 停止所有的Hadoop守护进程。包括NameNode、 Secondary NameNode、DataNode、ResourceManager、NodeManager
sbin/start-dfs.sh 启动Hadoop HDFS守护进程NameNode、SecondaryNameNode、DataNode
sbin/stop-dfs.sh 停止Hadoop HDFS守护进程NameNode、SecondaryNameNode和DataNode
sbin/hadoop-daemons.sh start namenode 单独启动NameNode守护进程
sbin/hadoop-daemons.sh stop namenode 单独停止NameNode守护进程
sbin/hadoop-daemons.sh start datanode 单独启动DataNode守护进程
sbin/hadoop-daemons.sh stop datanode 单独停止DataNode守护进程
sbin/hadoop-daemons.sh start secondarynamenode 单独启动SecondaryNameNode守护进程
sbin/hadoop-daemons.sh stop secondarynamenode 单独停止SecondaryNameNode守护进程
sbin/start-yarn.sh 启动ResourceManager、NodeManager
sbin/stop-yarn.sh 停止ResourceManager、NodeManager
sbin/yarn-daemon.sh start resourcemanager 单独启动ResourceManager
sbin/yarn-daemons.sh start nodemanager 单独启动NodeManager
sbin/yarn-daemon.sh stop resourcemanager 单独停止ResourceManager
sbin/yarn-daemons.sh stopnodemanager 单独停止NodeManager
sbin/mr-jobhistory-daemon.sh start historyserver 手动启动jobhistory
sbin/mr-jobhistory-daemon.sh stop historyserver 手动停止jobhistory
```

## 参考文档

[Apache Hadoop 3.3.6 – Hadoop: Setting up a Single Node Cluster.](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation)

