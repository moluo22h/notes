## 	中间件安装

### installMariadb



/opt/bitnami/mariadb/conf/my.cnf

```mysql
[mysqld]
skip-name-resolve
explicit_defaults_for_timestamp
basedir=/opt/bitnami/mariadb
plugin_dir=/opt/bitnami/mariadb/plugin
port=3306
socket=/opt/bitnami/mariadb/tmp/mysql.sock
tmpdir=/opt/bitnami/mariadb/tmp
max_allowed_packet=16M
bind-address=0.0.0.0
pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid
log-error=/opt/bitnami/mariadb/logs/mysqld.log
character-set-server=UTF8
collation-server=utf8_general_ci
slow_query_log = 1
long_query_time = 2
slow_query_log_file = /tmp/slow_query.log

[client]
port=3306
socket=/opt/bitnami/mariadb/tmp/mysql.sock
default-character-set=UTF8
plugin_dir=/opt/bitnami/mariadb/plugin

[manager]
port=3306
socket=/opt/bitnami/mariadb/tmp/mysql.sock
pid-file=/opt/bitnami/mariadb/tmp/mysqld.pid
```

/bitnami/mariadb

```bash
mkdir /data/middleware/mariadb/

```







```bash
# 安装
docker pull bitnami/mariadb:10.5.12-debian-10-r0

docker run -d --name mariadb --restart always -e MARIADB_ROOT_PASSWORD=N2XRLTsJ2t -p 3306:3306 bitnami/mariadb:10.5.12-debian-10-r0


docker run -d --name mariadb --restart always -e MARIADB_ROOT_PASSWORD=N2XRLTsJ2t -p 3306:3306 -v /data/middleware/mariadb/data:/bitnami/mariadb/data bitnami/mariadb:10.5.12-debian-10-r0

docker run -d --name mariadb --restart always -e MARIADB_ROOT_PASSWORD=N2XRLTsJ2t -e MARIADB_DATABASE=my_database -e TZ='Asia/Shanghai' -p 3306:3306 bitnami/mariadb:10.5.12-debian-10-r0

docker run -d --name mariadb --restart always -e MARIADB_ROOT_PASSWORD=N2XRLTsJ2t -e MARIADB_DATABASE=my_database -e TZ='Asia/Shanghai' -p 3306:3306 -v /data/middleware/mariadb/conf/my.cnf:/opt/bitnami/mariadb/conf/my.cnf -v /data/middleware/mariadb/data:/bitnami/mariadb/data bitnami/mariadb:10.5.12-debian-10-r0

# 初始化数据
mysql -h 127.0.0.1 -uroot -pN2XRLTsJ2t
create database if not exists aiworks;
use aiworks;
source /root/nebula/backend/datascience-service/src/main/resources/sql/aiworks.sql;
source /root/nebula/onebox/business/backend/aiworks.backend.initialize.mariadb.addTestUser.sql;


create database if not exists graph_analysis;


mysqladmin status -uroot -p"${password_aux}"





docker run -d --name mariadb --restart always -e MARIADB_ROOT_PASSWORD=N2XRLTsJ2t -p 3306:3306 bitnami/mariadb:10.5.19-debian-11-r29

docker run -d --name mariadb --restart always -e MARIADB_ROOT_PASSWORD=N2XRLTsJ2t -p 3306:3306 -v /data/middleware/mariadb/data:/bitnami/mariadb/data bitnami/mariadb:10.5.19-debian-11-r29

docker run -d --name mariadb --restart always -e MARIADB_ROOT_PASSWORD=N2XRLTsJ2t -e MARIADB_DATABASE=my_database -e TZ='Asia/Shanghai' -p 3306:3306 -v /data/middleware/mariadb/conf/my.cnf:/opt/bitnami/mariadb/conf/my.cnf -v /data/middleware/mariadb/data:/bitnami/mariadb/data bitnami/mariadb:10.5.19-debian-11-r29
```



### installCassandra

```bash
# 安装
docker pull bitnami/cassandra:3.11.11-debian-10-r4
docker run -d --name cassandra --restart always -p 9042:9042 -e CASSANDRA_USER=cassandra -e CASSANDRA_PASSWORD=TS7syEyYa9 -v /data/middleware/cassandra/data:/bitnami/cassandra/data bitnami/cassandra:3.11.11-debian-10-r4


docker run -d --name cassandra --restart always -p 9042:9042 -e CASSANDRA_USER=cassandra -e CASSANDRA_PASSWORD=TS7syEyYa9 bitnami/cassandra:3.11.11-debian-10-r4
```



### installRedis

```bash
# 安装
docker pull bitnami/redis:6.2.2-debian-10-r0
docker run -d --name redis --restart always -p 6379:6379 -e REDIS_PASSWORD=wKffDkYpwS bitnami/redis:6.2.2-debian-10-r0


docker run -d --name redis --restart always -p 6379:6379 -e REDIS_PASSWORD=wKffDkYpwS -v /data/middleware/redis/data:/bitnami/redis/data bitnami/redis:6.2.2-debian-10-r0
```



### installMinio

```bash
# 安装
docker pull bitnami/minio:2022.8.22-debian-11-r0
docker run -d --name minio --restart always -p 9000:9000 -p 9001:9001 -e MINIO_ROOT_USER=admin -e MINIO_ROOT_PASSWORD=Qc8ydPvzq2 bitnami/minio:2022.8.22-debian-11-r0


mkdir /data/middleware/minio/data
docker run -d --name minio --privileged=true --restart always -p 9000:9000 -p 9001:9001 -e MINIO_ROOT_USER=admin -e MINIO_ROOT_PASSWORD=Qc8ydPvzq2 -v /data/middleware/minio/data:/data bitnami/minio:2022.8.22-debian-11-r0

chown -R 1001:1001 /data/middleware/minio/data

# 初始化数据
onebox/business/backend/aiworks.backend.initialize.minio.sh
onebox/business/algo/aiworks.algo.initialize.minio.sh
```

参考：[【Minio】Linux中Docker下Minio启动提示权限不足-CSDN博客](https://blog.csdn.net/qq_37274323/article/details/133340512)



### installPostgresql

/dev/shm





/bitnami/postgresql





```bash 
# 安装
docker pull bitnami/postgresql:14.4.0-debian-11-r9
docker run -d --name postgresql --restart always -p 5432:5432 -e POSTGRESQL_USERNAME=postgres -e POSTGRESQL_PASSWORD=UbGJR1ymQC bitnami/postgresql:14.4.0-debian-11-r9

# 初始化数据
psql --host 127.0.0.1 -U postgres -d postgres -p 5432
create database aiworks;
psql --host 127.0.0.1 -U postgres -d postgres -p 5432 -d aiworks -f /root/nebula/onebox/business/backend/aiworks.backend.initialize.postgresql.sql


psql --host 127.0.0.1 -U postgres -d postgres -p 5432 -d aiworks -f /aiworks.sql







docker run -d --name postgresql --restart always -p 5432:5432 -e POSTGRESQL_PORT_NUMBER=5432 -e POSTGRESQL_VOLUME_DIR=/bitnami/postgresql -e PGDATA=/bitnami/postgresql/data -e POSTGRES_PASSWORD=UbGJR1ymQC -e POSTGRESQL_ENABLE_LDAP=no -e POSTGRESQL_ENABLE_TLS=no -e POSTGRESQL_LOG_HOSTNAME=false -e POSTGRESQL_LOG_CONNECTIONS=false -e POSTGRESQL_LOG_DISCONNECTIONS=false  -e POSTGRESQL_PGAUDIT_LOG_CATALOG=off -e POSTGRESQL_CLIENT_MIN_MESSAGES=error -e POSTGRESQL_SHARED_PRELOAD_LIBRARIES=pgaudit -v /data/middleware/postgresql/data:/bitnami/postgresql/data bitnami/postgresql:14.4.0-debian-11-r9-20220729


docker run -d --name postgresql --restart always -p 5432:5432 -e POSTGRESQL_PORT_NUMBER=5432 -e POSTGRESQL_VOLUME_DIR=/bitnami/postgresql -e PGDATA=/bitnami/postgresql/data -e POSTGRES_PASSWORD=UbGJR1ymQC -e POSTGRESQL_ENABLE_LDAP=no -e POSTGRESQL_ENABLE_TLS=no -e POSTGRESQL_LOG_HOSTNAME=false -e POSTGRESQL_LOG_CONNECTIONS=false -e POSTGRESQL_LOG_DISCONNECTIONS=false  -e POSTGRESQL_PGAUDIT_LOG_CATALOG=off -e POSTGRESQL_CLIENT_MIN_MESSAGES=error -e POSTGRESQL_SHARED_PRELOAD_LIBRARIES=pgaudit -v /data/middleware/postgresql/shm/dshm:/dev/shm/dshm -v /data/middleware/postgresql/data:/bitnami/postgresql/data bitnami/postgresql:14.4.0-debian-11-r9
```

[Docker 配置 PostgreSQL12 以及 plpython3u拓展 - Ioku的博客](https://ioku.net/index.php/archives/1497/)

```bash
docker exec -it --user root  postgresql bash
apt-get update && apt-get install postgresql-plpython3-13
CREATE PROCEDURAL LANGUAGE plpython3u;
```





### installJupyterhub

/usr/local/etc/jupyterhub/jupyterhub_config.py

```python
import glob
import os
import re
import sys

from binascii import a2b_hex

from tornado.httpclient import AsyncHTTPClient
from kubernetes import client
from jupyterhub.utils import url_path_join

# Make sure that modules placed in the same directory as the jupyterhub config are added to the pythonpath
configuration_directory = os.path.dirname(os.path.realpath(__file__))
sys.path.insert(0, configuration_directory)

from z2jh import (
    get_config,
    set_config_if_not_none,
    get_name,
    get_name_env,
    get_secret_value,
)


def camelCaseify(s):
    """convert snake_case to camelCase

    For the common case where some_value is set from someValue
    so we don't have to specify the name twice.
    """
    return re.sub(r"_([a-z])", lambda m: m.group(1).upper(), s)


# Configure JupyterHub to use the curl backend for making HTTP requests,
# rather than the pure-python implementations. The default one starts
# being too slow to make a large number of requests to the proxy API
# at the rate required.
AsyncHTTPClient.configure("tornado.curl_httpclient.CurlAsyncHTTPClient")

c.JupyterHub.spawner_class = "kubespawner.KubeSpawner"

# Connect to a proxy running in a different pod. Note that *_SERVICE_*
# environment variables are set by Kubernetes for Services
c.ConfigurableHTTPProxy.api_url = (
    f'http://{get_name("proxy-api")}:{get_name_env("proxy-api", "_SERVICE_PORT")}'
)
c.ConfigurableHTTPProxy.should_start = False

# Do not shut down user pods when hub is restarted
c.JupyterHub.cleanup_servers = False

# Check that the proxy has routes appropriately setup
c.JupyterHub.last_activity_interval = 60

# Don't wait at all before redirecting a spawning user to the progress page
c.JupyterHub.tornado_settings = {
    "slow_spawn_timeout": 0,
}


# configure the hub db connection
db_type = get_config("hub.db.type")
if db_type == "sqlite-pvc":
    c.JupyterHub.db_url = "sqlite:///jupyterhub.sqlite"
elif db_type == "sqlite-memory":
    c.JupyterHub.db_url = "sqlite://"
else:
    set_config_if_not_none(c.JupyterHub, "db_url", "hub.db.url")
db_password = get_secret_value("hub.db.password", None)
if db_password is not None:
    if db_type == "mysql":
        os.environ["MYSQL_PWD"] = db_password
    elif db_type == "postgres":
        os.environ["PGPASSWORD"] = db_password
    else:
        print(f"Warning: hub.db.password is ignored for hub.db.type={db_type}")


# c.JupyterHub configuration from Helm chart's configmap
for trait, cfg_key in (
    ("concurrent_spawn_limit", None),
    ("active_server_limit", None),
    ("base_url", None),
    ("allow_named_servers", None),
    ("named_server_limit_per_user", None),
    ("authenticate_prometheus", None),
    ("redirect_to_server", None),
    ("shutdown_on_logout", None),
    ("template_paths", None),
    ("template_vars", None),
):
    if cfg_key is None:
        cfg_key = camelCaseify(trait)
    set_config_if_not_none(c.JupyterHub, trait, "hub." + cfg_key)

# hub_bind_url configures what the JupyterHub process within the hub pod's
# container should listen to.
hub_container_port = 8081
c.JupyterHub.hub_bind_url = f"http://:{hub_container_port}"

# hub_connect_url is the URL for connecting to the hub for use by external
# JupyterHub services such as the proxy. Note that *_SERVICE_* environment
# variables are set by Kubernetes for Services.
c.JupyterHub.hub_connect_url = (
    f'http://{get_name("hub")}:{get_name_env("hub", "_SERVICE_PORT")}'
)

# implement common labels
# this duplicates the jupyterhub.commonLabels helper
common_labels = c.KubeSpawner.common_labels = {}
common_labels["app"] = get_config(
    "nameOverride",
    default=get_config("Chart.Name", "jupyterhub"),
)
common_labels["heritage"] = "jupyterhub"
chart_name = get_config("Chart.Name")
chart_version = get_config("Chart.Version")
if chart_name and chart_version:
    common_labels["chart"] = "{}-{}".format(
        chart_name,
        chart_version.replace("+", "_"),
    )
release = get_config("Release.Name")
if release:
    common_labels["release"] = release

c.KubeSpawner.namespace = os.environ.get("POD_NAMESPACE", "default")

# Max number of consecutive failures before the Hub restarts itself
# requires jupyterhub 0.9.2
set_config_if_not_none(
    c.Spawner,
    "consecutive_failure_limit",
    "hub.consecutiveFailureLimit",
)

for trait, cfg_key in (
    ("pod_name_template", None),
    ("start_timeout", None),
    ("image_pull_policy", "image.pullPolicy"),
    # ('image_pull_secrets', 'image.pullSecrets'), # Managed manually below
    ("events_enabled", "events"),
    ("extra_labels", None),
    ("extra_annotations", None),
    ("uid", None),
    ("fs_gid", None),
    ("service_account", "serviceAccountName"),
    ("storage_extra_labels", "storage.extraLabels"),
    # ("tolerations", "extraTolerations"), # Managed manually below
    ("node_selector", None),
    ("node_affinity_required", "extraNodeAffinity.required"),
    ("node_affinity_preferred", "extraNodeAffinity.preferred"),
    ("pod_affinity_required", "extraPodAffinity.required"),
    ("pod_affinity_preferred", "extraPodAffinity.preferred"),
    ("pod_anti_affinity_required", "extraPodAntiAffinity.required"),
    ("pod_anti_affinity_preferred", "extraPodAntiAffinity.preferred"),
    ("lifecycle_hooks", None),
    ("init_containers", None),
    ("extra_containers", None),
    ("mem_limit", "memory.limit"),
    ("mem_guarantee", "memory.guarantee"),
    ("cpu_limit", "cpu.limit"),
    ("cpu_guarantee", "cpu.guarantee"),
    ("extra_resource_limits", "extraResource.limits"),
    ("extra_resource_guarantees", "extraResource.guarantees"),
    ("environment", "extraEnv"),
    ("profile_list", None),
    ("extra_pod_config", None),
):
    if cfg_key is None:
        cfg_key = camelCaseify(trait)
    set_config_if_not_none(c.KubeSpawner, trait, "singleuser." + cfg_key)

image = get_config("singleuser.image.name")
if image:
    tag = get_config("singleuser.image.tag")
    if tag:
        image = "{}:{}".format(image, tag)

    c.KubeSpawner.image = image

# Combine imagePullSecret.create (single), imagePullSecrets (list), and
# singleuser.image.pullSecrets (list).
image_pull_secrets = []
if get_config("imagePullSecret.automaticReferenceInjection") and get_config(
    "imagePullSecret.create"
):
    image_pull_secrets.append(get_name("image-pull-secret"))
if get_config("imagePullSecrets"):
    image_pull_secrets.extend(get_config("imagePullSecrets"))
if get_config("singleuser.image.pullSecrets"):
    image_pull_secrets.extend(get_config("singleuser.image.pullSecrets"))
if image_pull_secrets:
    c.KubeSpawner.image_pull_secrets = image_pull_secrets

# scheduling:
if get_config("scheduling.userScheduler.enabled"):
    c.KubeSpawner.scheduler_name = get_name("user-scheduler")
if get_config("scheduling.podPriority.enabled"):
    c.KubeSpawner.priority_class_name = get_name("priority")

# add node-purpose affinity
match_node_purpose = get_config("scheduling.userPods.nodeAffinity.matchNodePurpose")
if match_node_purpose:
    node_selector = dict(
        matchExpressions=[
            dict(
                key="hub.jupyter.org/node-purpose",
                operator="In",
                values=["user"],
            )
        ],
    )
    if match_node_purpose == "prefer":
        c.KubeSpawner.node_affinity_preferred.append(
            dict(
                weight=100,
                preference=node_selector,
            ),
        )
    elif match_node_purpose == "require":
        c.KubeSpawner.node_affinity_required.append(node_selector)
    elif match_node_purpose == "ignore":
        pass
    else:
        raise ValueError(
            "Unrecognized value for matchNodePurpose: %r" % match_node_purpose
        )

# Combine the common tolerations for user pods with singleuser tolerations
scheduling_user_pods_tolerations = get_config("scheduling.userPods.tolerations", [])
singleuser_extra_tolerations = get_config("singleuser.extraTolerations", [])
tolerations = scheduling_user_pods_tolerations + singleuser_extra_tolerations
if tolerations:
    c.KubeSpawner.tolerations = tolerations

# Configure dynamically provisioning pvc
storage_type = get_config("singleuser.storage.type")
if storage_type == "dynamic":
    pvc_name_template = get_config("singleuser.storage.dynamic.pvcNameTemplate")
    c.KubeSpawner.pvc_name_template = pvc_name_template
    volume_name_template = get_config("singleuser.storage.dynamic.volumeNameTemplate")
    c.KubeSpawner.storage_pvc_ensure = True
    set_config_if_not_none(
        c.KubeSpawner, "storage_class", "singleuser.storage.dynamic.storageClass"
    )
    set_config_if_not_none(
        c.KubeSpawner,
        "storage_access_modes",
        "singleuser.storage.dynamic.storageAccessModes",
    )
    set_config_if_not_none(
        c.KubeSpawner, "storage_capacity", "singleuser.storage.capacity"
    )

    # Add volumes to singleuser pods
    c.KubeSpawner.volumes = [
        {
            "name": volume_name_template,
            "persistentVolumeClaim": {"claimName": pvc_name_template},
        }
    ]
    c.KubeSpawner.volume_mounts = [
        {
            "mountPath": get_config("singleuser.storage.homeMountPath"),
            "name": volume_name_template,
        }
    ]
elif storage_type == "static":
    pvc_claim_name = get_config("singleuser.storage.static.pvcName")
    c.KubeSpawner.volumes = [
        {"name": "home", "persistentVolumeClaim": {"claimName": pvc_claim_name}}
    ]

    c.KubeSpawner.volume_mounts = [
        {
            "mountPath": get_config("singleuser.storage.homeMountPath"),
            "name": "home",
            "subPath": get_config("singleuser.storage.static.subPath"),
        }
    ]

# Inject singleuser.extraFiles as volumes and volumeMounts with data loaded from
# the dedicated k8s Secret prepared to hold the extraFiles actual content.
extra_files = get_config("singleuser.extraFiles", {})
if extra_files:
    volume = {
        "name": "files",
    }
    items = []
    for file_key, file_details in extra_files.items():
        # Each item is a mapping of a key in the k8s Secret to a path in this
        # abstract volume, the goal is to enable us to set the mode /
        # permissions only though so we don't change the mapping.
        item = {
            "key": file_key,
            "path": file_key,
        }
        if "mode" in file_details:
            item["mode"] = file_details["mode"]
        items.append(item)
    volume["secret"] = {
        "secretName": get_name("singleuser"),
        "items": items,
    }
    c.KubeSpawner.volumes.append(volume)

    volume_mounts = []
    for file_key, file_details in extra_files.items():
        volume_mounts.append(
            {
                "mountPath": file_details["mountPath"],
                "subPath": file_key,
                "name": "files",
            }
        )
    c.KubeSpawner.volume_mounts.extend(volume_mounts)

# Inject extraVolumes / extraVolumeMounts
c.KubeSpawner.volumes.extend(get_config("singleuser.storage.extraVolumes", []))
c.KubeSpawner.volume_mounts.extend(
    get_config("singleuser.storage.extraVolumeMounts", [])
)

c.JupyterHub.services = []

if get_config("cull.enabled", False):
    cull_cmd = ["python3", "-m", "jupyterhub_idle_culler"]
    base_url = c.JupyterHub.get("base_url", "/")
    cull_cmd.append("--url=http://localhost:8081" + url_path_join(base_url, "hub/api"))

    cull_timeout = get_config("cull.timeout")
    if cull_timeout:
        cull_cmd.append("--timeout=%s" % cull_timeout)

    cull_every = get_config("cull.every")
    if cull_every:
        cull_cmd.append("--cull-every=%s" % cull_every)

    cull_concurrency = get_config("cull.concurrency")
    if cull_concurrency:
        cull_cmd.append("--concurrency=%s" % cull_concurrency)

    if get_config("cull.users"):
        cull_cmd.append("--cull-users")

    if get_config("cull.removeNamedServers"):
        cull_cmd.append("--remove-named-servers")

    cull_max_age = get_config("cull.maxAge")
    if cull_max_age:
        cull_cmd.append("--max-age=%s" % cull_max_age)

    c.JupyterHub.services.append(
        {
            "name": "cull-idle",
            "admin": True,
            "command": cull_cmd,
        }
    )

for key, service in get_config("hub.services", {}).items():
    # c.JupyterHub.services is a list of dicts, but
    # hub.services is a dict of dicts to make the config mergable
    service.setdefault("name", key)

    # As the api_token could be exposed in hub.existingSecret, we need to read
    # it it from there or fall back to the chart managed k8s Secret's value.
    service.pop("apiToken", None)
    service["api_token"] = get_secret_value(f"hub.services.{key}.apiToken")

    c.JupyterHub.services.append(service)


set_config_if_not_none(c.Spawner, "cmd", "singleuser.cmd")
set_config_if_not_none(c.Spawner, "default_url", "singleuser.defaultUrl")

cloud_metadata = get_config("singleuser.cloudMetadata", {})

if cloud_metadata.get("blockWithIptables") == True:
    # Use iptables to block access to cloud metadata by default
    network_tools_image_name = get_config("singleuser.networkTools.image.name")
    network_tools_image_tag = get_config("singleuser.networkTools.image.tag")
    ip_block_container = client.V1Container(
        name="block-cloud-metadata",
        image=f"{network_tools_image_name}:{network_tools_image_tag}",
        command=[
            "iptables",
            "-A",
            "OUTPUT",
            "-d",
            cloud_metadata.get("ip", "169.254.169.254"),
            "-j",
            "DROP",
        ],
        security_context=client.V1SecurityContext(
            privileged=True,
            run_as_user=0,
            capabilities=client.V1Capabilities(add=["NET_ADMIN"]),
        ),
    )

    c.KubeSpawner.init_containers.append(ip_block_container)


if get_config("debug.enabled", False):
    c.JupyterHub.log_level = "DEBUG"
    c.Spawner.debug = True

# load /usr/local/etc/jupyterhub/jupyterhub_config.d config files
config_dir = "/usr/local/etc/jupyterhub/jupyterhub_config.d"
if os.path.isdir(config_dir):
    for file_path in sorted(glob.glob(f"{config_dir}/*.py")):
        file_name = os.path.basename(file_path)
        print(f"Loading {config_dir} config: {file_name}")
        with open(file_path) as f:
            file_content = f.read()
        # compiling makes debugging easier: https://stackoverflow.com/a/437857
        exec(compile(source=file_content, filename=file_name, mode="exec"))

# load potentially seeded secrets
#
# NOTE: ConfigurableHTTPProxy.auth_token is set through an environment variable
#       that is set using the chart managed secret.
c.JupyterHub.cookie_secret = get_secret_value("hub.config.JupyterHub.cookie_secret")
# NOTE: CryptKeeper.keys should be a list of strings, but we have encoded as a
#       single string joined with ; in the k8s Secret.
#
c.CryptKeeper.keys = get_secret_value("hub.config.CryptKeeper.keys").split(";")

# load hub.config values, except potentially seeded secrets already loaded
for app, cfg in get_config("hub.config", {}).items():
    if app == "JupyterHub":
        cfg.pop("proxy_auth_token", None)
        cfg.pop("cookie_secret", None)
        cfg.pop("services", None)
    elif app == "ConfigurableHTTPProxy":
        cfg.pop("auth_token", None)
    elif app == "CryptKeeper":
        cfg.pop("keys", None)
    c[app].update(cfg)

# execute hub.extraConfig entries
for key, config_py in sorted(get_config("hub.extraConfig", {}).items()):
    print("Loading extra config: %s" % key)
    exec(config_py)
```

/usr/local/etc/jupyterhub/z2jh.py

```python
"""
Utility methods for use in jupyterhub_config.py and dynamic subconfigs.

Methods here can be imported by extraConfig in values.yaml
"""
from collections import Mapping
from functools import lru_cache
import os

import yaml

# memoize so we only load config once
@lru_cache()
def _load_config():
    """Load the Helm chart configuration used to render the Helm templates of
    the chart from a mounted k8s Secret, and merge in values from an optionally
    mounted secret (hub.existingSecret)."""

    cfg = {}
    for source in ("secret/values.yaml", "existing-secret/values.yaml"):
        path = f"/usr/local/etc/jupyterhub/{source}"
        if os.path.exists(path):
            print(f"Loading {path}")
            with open(path) as f:
                values = yaml.safe_load(f)
            cfg = _merge_dictionaries(cfg, values)
        else:
            print(f"No config at {path}")
    return cfg


@lru_cache()
def _get_config_value(key):
    """Load value from the k8s ConfigMap given a key."""

    path = f"/usr/local/etc/jupyterhub/config/{key}"
    if os.path.exists(path):
        with open(path) as f:
            return f.read()
    else:
        raise Exception(f"{path} not found!")


@lru_cache()
def get_secret_value(key, default="never-explicitly-set"):
    """Load value from the user managed k8s Secret or the default k8s Secret
    given a key."""

    for source in ("existing-secret", "secret"):
        path = f"/usr/local/etc/jupyterhub/{source}/{key}"
        if os.path.exists(path):
            with open(path) as f:
                return f.read()
    if default != "never-explicitly-set":
        return default
    raise Exception(f"{key} not found in either k8s Secret!")


def get_name(name):
    """Returns the fullname of a resource given its short name"""
    return _get_config_value(name)


def get_name_env(name, suffix=""):
    """Returns the fullname of a resource given its short name along with a
    suffix, converted to uppercase with dashes replaced with underscores. This
    is useful to reference named services associated environment variables, such
    as PROXY_PUBLIC_SERVICE_PORT."""
    env_key = _get_config_value(name) + suffix
    env_key = env_key.upper().replace("-", "_")
    return os.environ[env_key]


def _merge_dictionaries(a, b):
    """Merge two dictionaries recursively.

    Simplified From https://stackoverflow.com/a/7205107
    """
    merged = a.copy()
    for key in b:
        if key in a:
            if isinstance(a[key], Mapping) and isinstance(b[key], Mapping):
                merged[key] = _merge_dictionaries(a[key], b[key])
            else:
                merged[key] = b[key]
        else:
            merged[key] = b[key]
    return merged


def get_config(key, default=None):
    """
    Find a config item of a given name & return it

    Parses everything as YAML, so lists and dicts are available too

    get_config("a.b.c") returns config['a']['b']['c']
    """
    value = _load_config()
    # resolve path in yaml
    for level in key.split("."):
        if not isinstance(value, dict):
            # a parent is a scalar or null,
            # can't resolve full path
            return default
        if level not in value:
            return default
        else:
            value = value[level]
    return value


def set_config_if_not_none(cparent, name, key):
    """
    Find a config item of a given name, set the corresponding Jupyter
    configuration item if not None
    """
    data = get_config(key)
    if data is not None:
        setattr(cparent, name, data)
```



```python
c = get_config()
#c.Spawner.args = ['--allow-root']
#c.JupyterHub.authenticator_class = "dummy"
c.Authenticator.admin_users = {'root','admin'}
c.LocalAuthenticator.create_system_users=True
```



```bash
# 安装
docker pull jupyterhub/k8s-hub:1.2.0
docker run -d --name k8s-hub --restart always -p 8081:8081 -e JUPYTERHUB_USERNAME=admin -e JUPYTERHUB_PASSWORD=nebula@2021 jupyterhub/k8s-hub:1.2.0

docker run -d --name k8s-hub -p 8081:8081 -e JUPYTERHUB_USERNAME=admin -e JUPYTERHUB_PASSWORD=nebula@2021 -v ./jupyterhub_config.py:/usr/local/etc/jupyterhub/jupyterhub_config.py -v ./z2jh.py:/usr/local/etc/jupyterhub/z2jh.py jupyterhub/k8s-hub:1.2.0

docker run -d --name k8s-hub -p 8000:8000 -p 8081:8081 -e JUPYTERHUB_USERNAME=admin -e JUPYTERHUB_PASSWORD=nebula@2021 -v ./jupyterhub:/usr/local/etc/jupyterhub jupyterhub/k8s-hub:1.2.0


docker pull bitnami/jupyterhub:3.0.0-debian-11-r19
docker run -d --name jupyterhub --restart always -p 8000:8000 -e JUPYTERHUB_USERNAME=admin -e JUPYTERHUB_PASSWORD=nebula@2021 bitnami/jupyterhub:3.0.0-debian-11-r19

docker run -d --name jupyterhub -p 8000:8000 -e JUPYTERHUB_USERNAME=admin -e JUPYTERHUB_PASSWORD=nebula@2021 bitnami/jupyterhub:3.0.0-debian-11-r19

docker run -d --name jupyterhub -p 8000:8000 bitnami/jupyterhub:4.1.5-debian-12-r5





docker pull jupyterhub/jupyterhub:3.0
docker run -d --name jupyterhub --restart always -p 8000:8000 -p 8081:8081 -e JUPYTERHUB_USERNAME=admin -e JUPYTERHUB_PASSWORD=nebula@2021 jupyterhub/jupyterhub:3.0
docker exec -it jupyterhub bash
adduser admin
nebula@2021

pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple jupyter
pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple jupyterhub==1.5.1
pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple jupyterlab
pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple notebook==6.5.2



pip install -i https://pypi.tuna.tsinghua.edu.cn/simple jupyter
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple jupyterhub --upgrade
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple notebook --upgrade
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple jupyterlab

docker run -d --name jupyterhub --restart always -p 8000:8000 -p 8081:8081 jupyterhub/jupyterhub:1.5.1


docker run -d --name jupyterhub --restart always -p 8000:8000 -p 8081:8081 -e JUPYTERHUB_USERNAME=admin -e JUPYTERHUB_PASSWORD=nebula@2021 -v /data/middleware/jupyterhub/conf/jupyterhub_config.py:/etc/jupyterhub/jupyterhub_config.py jupyterhub/jupyterhub:1.5.1 jupyterhub -f=/etc/jupyterhub/jupyterhub_config.py --no-ssl


docker run -d --name jupyterhub --restart always -p 8000:8000 -p 8081:8081 -e JUPYTERHUB_USERNAME=admin -e JUPYTERHUB_PASSWORD=nebula@2021 -v /data/middleware/jupyterhub/conf/jupyterhub_config.py:/etc/jupyterhub/jupyterhub_config.py jupyterhub/jupyterhub:3.0 jupyterhub -f=/etc/jupyterhub/jupyterhub_config.py --no-ssl






docker run -d --name jupyterhub --restart always -p 8000:8000 -p 8081:8081 -e JUPYTERHUB_USERNAME=admin -e JUPYTERHUB_PASSWORD=nebula@2021 -v /data/middleware/jupyterhub/conf/jupyterhub_config.py:/etc/jupyterhub/jupyterhub_config.py jupyterhub/jupyterhub:4.1.5 jupyterhub -f=/etc/jupyterhub/jupyterhub_config.py --no-ssl
```

参考文档：[docker 搭建 python教学环境 jupyterhub + nbgrader 教程 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/450744191)





## 服务安装

### installAiworksAlgo

/app/config/config.yaml

```yaml
mysql:
  url: jdbc:mysql://172.17.0.1:3306/aiworks
  host: 172.17.0.1
  port: 3306
  user: root
  password: N2XRLTsJ2t
  dbname: aiworks
  driver: com.mysql.cj.jdbc.Driver
gp:
  url: jdbc:postgresql://172.17.0.1:5432/aiworks
  host: 172.17.0.1
  port: 5432
  user: postgres
  password: UbGJR1ymQC
  dbname: aiworks
  driver: org.postgresql.Driver
minio:
  address: "http://172.17.0.1:9000"
  access_key: "admin"
  secret_key: "Qc8ydPvzq2"
  connection: client
metrics_path_tmp: ml_model.
insight_server: https://jianwei.zjvis.net/api
model:
  path: static/model_demo
database:
  OLTP: mysql
  OLAP: greenplum
am_warehouse: /opt/file_models/
log_path: /app/logs
```



```bash
docker save insecure.docker.registry.local:80/aiworks-algo-flask:1.0.0-C7794598 > aiworks-algo-flask.tar
docker load < aiworks-algo-flask.tar


docker run -d --name aiworks-algo-flask --restart always -p 5000:5000 -v /data/nebula/algo/conf/config.yaml:/app/config/config.yaml -v /data/nebula/algo/data/file_models:/opt/file_models  insecure.docker.registry.local:80/aiworks-algo-flask:1.0.0-C7794598
```

测试是否部署成功

```bash
http://192.168.10.60:5000/healthy/liveness
```



### installAiworksBackend



初始化数据

```bash
```







/app/application.yaml

```yaml
app:
  server:
    port: 8080
  sampling: true
  socketio:
    port: 1080
    boss-count: 1
    work-count: 100
    allow-custom-requests: true
    upgrade-timeout: 1000000
    ping-timeout: 6000000
    ping-interval: 25000

restful:
  flaskServer:
    address: "http://172.17.0.1:5000"
  insightServer:
    address: "https://jianwei.zjvis.net/api"
    frontend: "https://jianwei.zjvis.net/"

upload:
  large-file-threshold: 50MB
  is-win: false
  chunk-size: 5242880
  folder-path: ./file_uploads
  expire-time: 1209600
  temp-file-path: ./csv_tmp/
  shp2pgsql-path: /usr/bin/shp2pgsql
  gis:
    file-path: ./gis_data/
  photo:
    resource-path: nebula
    path: vis/wordcloud/
  case:
    video-path: vis/caseVideo/
  model:
    folder-path: /opt/file_models/

spring:
  application:
    name: datascience
  servlet:
    multipart:
      max-request-size: 1GB
      max-file-size: 1GB
  output:
    ansi:
      enabled: always
  jackson:
    time-zone: GMT+8
  mvc:
    async:
      request-timeout: 600000
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    druid:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://172.17.0.1:3306/aiworks?useUnicode=true&characterEncoding=UTF-8&allowMultiQueries=true&useAffectedRows=true&serverTimezone=Asia/Shanghai&useSSL=false
      host: my-mariadb.middleware
      port: 3306
      username: root
      password: N2XRLTsJ2t
      keep-alive: true
      initial-size: 10
      max-active: 100
      max-wait: 500
      time-between-eviction-runs-millis: 60000
      max-evictable-idle-time-millis: 1800001
      min-evictable-idle-time-millis: 160000
      test-while-idle: true
      test-on-borrow: true
      test-on-return: false
      pool-prepared-statements: true
      max-pool-prepared-statement-per-connection-size: 10
      validation-query: select 1
  redis:
    database: 0
    host: 172.17.0.1
    port: 6379
    password: wKffDkYpwS
    timeout: 500

jwt:
  header: Authorization
  token-start-with: Bearer
  base64-secret: ZmQ0ZGI5NjQ0MDQwY2I4MjMxY2Y3ZmI3MjdhN2ZmMjNhODViOTg1ZGE0NTBjMGM4NDA5NzYxMjdjOWMwYWRmZTBlZjlhNGY3ZTg4Y2U3YTE1ODVkZDU5Y2Y3OGYwZWE1NzUzNWQ2YjFjZDc0NGMxZWU2MmQ3MjY1NzJmNTE0MzI=
  token-validity-in-seconds: 86400000
  online-key: online-token-
  code-key: code-key

rsa:
  privateKey: MIIBUwIBADANBgkqhkiG9w0BAQEFAASCAT0wggE5AgEAAkEA0vfvyTdGJkdbHkB8mp0f3FE0GYP3AYPaJF7jUd1M0XxFSE2ceK3k2kw20YvQ09NJKk+OMjWQl9WitG9pB6tSCQIDAQABAkA2SimBrWC2/wvauBuYqjCFwLvYiRYqZKThUS3MZlebXJiLB+Ue/gUifAAKIg1avttUZsHBHrop4qfJCwAI0+YRAiEA+W3NK/RaXtnRqmoUUkb59zsZUBLpvZgQPfj1MhyHDz0CIQDYhsAhPJ3mgS64NbUZmGWuuNKp5coY2GIj/zYDMJp6vQIgUueLFXv/eZ1ekgz2Oi67MNCk5jeTF2BurZqNLR3MSmUCIFT3Q6uHMtsB9Eha4u7hS31tj1UWE+D+ADzp59MGnoftAiBeHT7gDMuqeJHPL4b+kC+gzV4FGTfhR9q3tTbklZkD2A==
  publicKey: MFwwDQYJKoZIhvcNAQEBBQADSwAwSAJBANL378k3RiZHWx5AfJqdH9xRNBmD9wGD2iRe41HdTNF8RUhNnHit5NpMNtGL0NPTSSpPjjI1kJfVorRvaQerUgkCAwEAAQ==

shiro:
  cipher-key: f/SY5TIve5WWzT4aQlABJA==
  cookie:
    name: rememberMe
    max-age: 604800
    redis-key-prefix: online-user-info-

login:
  tianshu:
    key:
      private: MIICdgIBADANBgkqhkiG9w0BAQEFAASCAmAwggJcAgEAAoGBAIwWitG2Ia2fjqlZDy0VmGq4gtWrbvn9KqFuwut6E83JRbfAA4gkXQs7PTAEbhazSu3kCF0rDg-kmdQaqad9s2VleBAfvDk5wichHcPqoo9rK_zJcluBWJ9ftc9h5uLThbzumnbRF5eR9tznADLN0o4Wv7PMLWXoaqlamQme8uyfAgMBAAECgYB08nCrR8fvwO8A8ydXNNsL5MLci4RW0AGhyOySVlRoDCnWj0ajhe_i625WQqyA6OaZmC9fUA0qA_ijeCq_d5GlygPkQWLaUH0x23DvMUaLDNCcgcDsmdk4i3A2euwkzjN0iv3IHAwujUG8DsHr8ply-BVbdPVJVzWKnYc8xWbNyQJBANmabjDUS5wrJU-k7IlMpKXB5siYvBttc3BqAzbSVYFTGuLqioE6GXMH-x7mv9fs1RKVTXGhhjsSZvMYymggGrsCQQCkzpjkT9ky7TpUb-8ZBonruVqHCnib8wyoBlFGYU4b7F4fDEUdBAXDG8bZ1o2KTbRUeNfLCK76pQMTnAe8QXFtAkEAj3Xv5cNg4eHUJHD__PkJp7pxc5i2k4KSU--glNkQxEVM-YNVsyLhumPtnI7Wtf2O8ER8nUi3XWSheO3EK-fWlwJAecM6KtTjwECNK_1XRcIS_FoBjGwsF-xGmY2xVrJlpzPHhmDmXz2tlC1diWx_PoOSjCaMKLHNtdlcoIxTGr-vMQJARXzVwOESYHa23RQVoiFGbfTfWpyJoEQuQqkgRPNn1FGMDvpzWJiFy6uOSVddwkbKIrJCQg1Gcla6NDaiExyEvA
    salt: abcdefg
    code:
      timeout: 3600000

jasypt:
  encryptor:
    password: 4tr=Kl34jDs@O/u4_#2c

aliyun:
  access-key-id: XXXXXXXX
  access-key-secret: XXXXXXXX
  master-key-value: ZJLAB
  sms-magic-key: ZJVIS2022
  sms-white-list-phone-numbers: 18800000000,18700000000

logging:
  level:
    org.zjvis.datascience.service.mapper: WARN
    org.docx4j: WARN
    springfox.documentation.spring.web.readers.operation.CachingOperationNameGenerator: WARN
  config: classpath:logback-spring-prod.xml

swagger:
  push:
    enable: false

mybatis:
  mapper-locations: classpath*:mapper/*.xml
  type-aliases-package: org.zjvis.datascience.common.dto
  configuration:
    map-underscore-to-camel-case: true
  type-handlers-package: org.zjvis.datascience.common.handler

pagehelper:
  helperDialect: mysql
  reasonable: true
  supportMethodsArguments: true
  params: count=countSql
  returnPageInfo: check

postgres:
  port: 5432
  host: 172.17.0.1
  driver: org.postgresql.Driver
  username: postgres
  password: UbGJR1ymQC
  database: aiworks
  pool:
    init-size: 20
    max-active: 300
    min-idle: 10
    max-wait-time: 30000
    retry: 3

minio:
  url: "http://172.17.0.1:9000"
  secret: "admin"
  password: "Qc8ydPvzq2"
  bucket-size: 100MB

Jlab:
  homeDir: /root/flask/aiworks-py/common/
  notebookDir: /root/jupyterlab/notebooks/
  initLab: init_container.sh
  loadData: load_data.sh
  loadView: load_view.sh
  getToken: get_token.sh
  df2db: df_to_db.sh
  urlTemp: "http://xx.xxx.xx.xxx:%d/lab/workspaces/auto/tree/%s/?token=%s"
  userName: root
  password: some-password
  port: 22
  host: xx.xxx.xx.xxx

Jhub:
  host: http://172.17.0.1/jupyterhub
  adminUser: admin
  adminPassword: "nebula@2021"
  blockSize: 2097152
```





```bash
docker pull openjdk:11.0.14.1-jdk-bullseye


docker save insecure.docker.registry.local:80/aiworks-backend-springboot:1.0.0-C7794598 > aiworks-backend-springboot.tar
docker load < aiworks-backend-springboot.tar


docker run -d --name aiworks-backend-springboot --restart always -p 9100:8080 -e JAVA_OPS="-server -Xmx4096m -Xms1024m" -e APPLICATION_YAML_PATH="/app/application.yaml" -v /data/nebula/backend/conf/application.yaml:/app/application.yaml insecure.docker.registry.local:80/aiworks-backend-springboot:1.0.0-C7794598

docker run -d --name aiworks-backend-springboot -p 9100:8080 -e JAVA_OPS="-server -Xmx4096m -Xms1024m" -e APPLICATION_YAML_PATH="/app/application.yaml" -v /data/nebula/backend/conf/application.yaml:/app/application.yaml insecure.docker.registry.local:80/aiworks-backend-springboot:1.0.0-C7794598
```





### installAiworksFrontend

/etc/nginx/conf.d/default.conf

```nginx
server {
    listen       80;
    server_name  localhost;
    client_max_body_size 1000m;

    location / {
        root   /usr/share/nginx/html;
        try_files $uri $uri/ /index.html;
    }
}
```



```bash
docker save insecure.docker.registry.local:80/aiworks-frontend-vue:1.0.0-C7794598 > aiworks-frontend-vue.tar
docker load < aiworks-frontend-vue.tar

docker run -d --name aiworks-frontend-vue --restart always -p 8080:80 insecure.docker.registry.local:80/aiworks-frontend-vue:1.0.0-C7794598

docker run -d --name aiworks-frontend-vue --restart always -p 8080:80 -v /data/nebula/frontend/conf/default.conf:/etc/nginx/conf.d/default.conf insecure.docker.registry.local:80/aiworks-frontend-vue:1.0.0-C7794598
```



### installAiworksDocs

/etc/nginx/conf.d/default.conf

```nginx
server {
    listen       80;
    server_name  localhost;
    client_max_body_size 1000m;

    location / {
        root   /usr/share/nginx/html;
        try_files $uri $uri/ /index.html;
    }
}
```



```bash
docker save insecure.docker.registry.local:80/aiworks-docs-nginx:1.0.0-C7794598 > aiworks-docs-nginx.tar
docker load < aiworks-docs-nginx.tar

docker run -d --name aiworks-docs-nginx --restart always -p 8082:80 -v /data/nebula/docs/conf/default.conf:/etc/nginx/conf.d/default.conf insecure.docker.registry.local:80/aiworks-docs-nginx:1.0.0-C7794598
```



### installAiworksGraphAnalysisBackend

/app/application.yaml

```yaml
server:
  port: 8080
spring:
  application:
    name: graph-analysis
  mvc:
    async:
      request-timeout: 600000
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    druid:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://172.17.0.1:3306/graph_analysis?useUnicode=true&characterEncoding=UTF-8&allowMultiQueries=true&useAffectedRows=true&serverTimezone=Asia/Shanghai
      username: root
      password: N2XRLTsJ2t
      initial-size: 10
      max-active: 50
      max-wait: 5000
      time-between-eviction-runs-millis: 30000
      min-evictable-idle-time-millis: 100000
      test-while-idle: true
      test-on-borrow: true
      test-on-return: false
      pool-prepared-statements: true
      max-pool-prepared-statement-per-connection-size: 10
  sql:
    init:
      schema-locations: classpath:sql/init.sql
      mode: always
mybatis:
  mapper-locations: classpath*:mapper/*.xml
  type-aliases-package: org.zjvis.graph.analysis.service.dto
  configuration:
    map-underscore-to-camel-case: true
minio:
  http: "http://172.17.0.1:9000"
  key: "admin"
  psw: "Qc8ydPvzq2"
  bucket-size: 100MB
janusgraph:
  gremlin:
    graph: org.janusgraph.core.JanusGraphFactory
  storage:
    backend: cql
    hostname: 172.17.0.1
    port: 9042
    username: cassandra
    password: cassandra
    cql:
      keyspace: zjvis_graph_analysis_dev
  cache:
    db-cache: true
    db-cache-clean-wait: 20
    db-cache-time: 180000
    db-cache-size: 0.25
  query:
    batch: true
    batch-property-prefetch: true
restful:
  dataScienceServer:
    address: http://172.17.0.1:9100
plugin:
  route: /graph-analysis
```





```bash
docker save insecure.docker.registry.local:80/aiworks-graph-analysis-backend-springboot:1.0.0-C7794598 > aiworks-graph-analysis-backend-springboot.tar
docker load < aiworks-graph-analysis-backend-springboot.tar



docker run -d --name aiworks-graph-analysis-backend-springboot --restart always -p 9101:8080 -e JAVA_OPS="-server -Xmx4096m -Xms1024m" -e APPLICATION_YAML_PATH="/app/application.yaml" -v /data/nebula/graph-analysis-backend/conf/application.yaml:/app/application.yaml insecure.docker.registry.local:80/aiworks-graph-analysis-backend-springboot:1.0.0-C7794598

docker run -d --name aiworks-graph-analysis-backend-springboot -p 9101:8080 -e JAVA_OPS="-server -Xmx4096m -Xms1024m" -e APPLICATION_YAML_PATH="/app/application.yaml" -v /data/nebula/graph-analysis-backend/conf/application.yaml:/app/application.yaml insecure.docker.registry.local:80/aiworks-graph-analysis-backend-springboot:1.0.0-C7794598
```





```bash
http://192.168.10.60:8080/plugin/checkHealth
```





### installAiworksGraphAnalysisFrontend

/etc/nginx/conf.d/default.conf

```nginx
server {
    listen       80;
    server_name  localhost;
    client_max_body_size 1000m;

    location / {
        root   /usr/share/nginx/html;
        try_files $uri $uri/ /index.html;
    }
}
```



```bash
docker save insecure.docker.registry.local:80/aiworks-graph-analysis-frontend-vue:1.0.0-C7794598 > aiworks-graph-analysis-frontend-vue.tar
docker load < aiworks-graph-analysis-frontend-vue.tar

docker run -d --name aiworks-graph-analysis-frontend-vue --restart always -p 8083:80  -v /data/nebula/graph-analysis-frontend/conf/default.conf:/etc/nginx/conf.d/default.conf insecure.docker.registry.local:80/aiworks-graph-analysis-frontend-vue:1.0.0-C7794598
```



## Nginx安装

### installNginx

```bash
docker run --name nginx -d -p 8080:80 some-content-nginx
```







## 其他命令

tar压缩

```bash
tar -cvf archive.tar foo bar
mv data.tar /tmp/

kubectl cp -n middleware my-mariadb-0:tmp/data.tar /tmp/data.tar
mv /tmp/data.tar /data.tar

docker cp kind-control-plane:/data.tar /

tar -xvf archive.tar -C /destination/folder

```



```bash
docker save insecure.docker.registry.local:80/aiworks-backend-springboot:1.0.0-C7794598 > aiworks-backend-springboot.tar
```

