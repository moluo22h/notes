# Item Pipeline

Spider抓取了一个`Item`后，将其发送到`Item Pipeline`，`Item Pipeline`将依次调用一个或多个`Item Pipeline组件`对其进行处理。

每个`item pipeline组件`（有时也简称为“item pipeline”）都是一个具有简单方法的Python类。他们接收到一个item后对其执行操作，并决定该item是继续传递到下一个`item pipeline组件`还是直接删除不再处理。

item pipelines的典型用途有：

- 清理HTML数据
- 验证抓取的数据（检查item是否包含某些字段）
- 检查重复项（并将其删除）
- 将抓取到的item存储在数据库中

## 编写自己的 item pipeline

每个item pipeline 组件都是一个Python类，必须实现以下方法：

- `process_item`(*self*, *item*, *spider*)

  每个item pipeline 组件都调用此方法。

  item 是一个 [item object](https://docs.scrapy.org/en/latest/topics/items.html#item-types), 请参考[Supporting All Item Types](https://docs.scrapy.org/en/latest/topics/items.html#supporting-item-types).

  [`process_item()`](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#process_item) 必须执行以下操作之一: 返回 [item object](https://docs.scrapy.org/en/latest/topics/items.html#item-types)，返回 [`Deferred`](https://twistedmatrix.com/documents/current/api/twisted.internet.defer.Deferred.html) 或抛出 [`DropItem`](https://docs.scrapy.org/en/latest/topics/exceptions.html#scrapy.exceptions.DropItem) 异常。

  删除的项目不再被其他item pipeline 组件处理。

  参数说明：

  - **item** ([item object](https://docs.scrapy.org/en/latest/topics/items.html#item-types)) – 抓取的item
  - **spider** ([`Spider`](https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider) 对象) – 抓取的item的spider

此外，他们还可以实现以下方法：

- `open_spider`(*self*, *spider*)

  打开spider后将调用此方法。

  参数说明：

  - **spider** ([`Spider`](https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider) 对象) – 被打开的spider

- `close_spider`(*self*, *spider*)

  关闭spider 时将调用此方法。

  参数说明：

  - **spider** ([`Spider`](https://docs.scrapy.org/en/latest/topics/spiders.html#scrapy.spiders.Spider) 对象) – 被关闭的spider

- `from_crawler`(*cls*, *crawler*)

  如果存在，则调用该类方法从 [`Crawler`](https://docs.scrapy.org/en/latest/topics/api.html#scrapy.crawler.Crawler)创建 pipeline实例。它必须返回pipeline的新实例。 Crawler 对象提供对所有Scrapy核心组件（如设置和信号）的访问； it is a way for pipeline to access them and hook its functionality into Scrapy.

  参数说明：

  - **crawler** ([`Crawler`](https://docs.scrapy.org/en/latest/topics/api.html#scrapy.crawler.Crawler) 对象) – 使用此管道的crawler 

## Item pipeline 示例

### 验证价格并且删除无价格的item

让我们看看下面的hypothetical  pipeline ，该pipeline 调整那些包含VAT（`price_excludes_vat` 属性）的items 的`price` 属性，并删除那些不包含`price` 的item：

```python
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem
class PricePipeline:

    vat_factor = 1.15

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        if adapter.get('price'):
            if adapter.get('price_excludes_vat'):
                adapter['price'] = adapter['price'] * self.vat_factor
            return item
        else:
            raise DropItem(f"Missing price in {item}")
```

### 将item写入JSON文件

以下pipeline 将所有抓取的items（来自各Spider）存储到单个 `items.jl` 文件中，每行包含一个以JSON格式序列化的item ：

```python
import json

from itemadapter import ItemAdapter

class JsonWriterPipeline:

    def open_spider(self, spider):
        self.file = open('items.jl', 'w')

    def close_spider(self, spider):
        self.file.close()

    def process_item(self, item, spider):
        line = json.dumps(ItemAdapter(item).asdict()) + "\n"
        self.file.write(line)
        return item
```

> 注意：JsonWriterPipeline的目的只是介绍如何编写 item pipelines。如果您真的要将所有已抓取的item存储到JSON文件中，则应使用[Feed exports](https://docs.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports)。

### 将item写入MongoDB

在此示例中，我们将使用 [pymongo](https://api.mongodb.com/python/current/)将item写入[MongoDB](https://www.mongodb.com/)。在Scrapy设置中指定了MongoDB地址和数据库名称； MongoDB集合以item 类命名。

该示例的主要目的是演示如何使用 [`from_crawler()`](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#from_crawler) 方法以及如何正确清理资源。

```python
import pymongo
from itemadapter import ItemAdapter

class MongoPipeline:

    collection_name = 'scrapy_items'

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def close_spider(self, spider):
        self.client.close()

    def process_item(self, item, spider):
        self.db[self.collection_name].insert_one(ItemAdapter(item).asdict())
        return item
```

### 建立item的快照

本示例演示了如何在 [`process_item()`](https://docs.scrapy.org/en/latest/topics/item-pipeline.html#process_item) 方法中使用 [coroutine syntax](https://docs.scrapy.org/en/latest/topics/coroutines.html) 。

该 item pipeline 向本地运行的 [Splash](https://splash.readthedocs.io/en/stable/) 实例发出请求，以呈现该itemURL的快照。下载请求响应后，item pipeline将快照保存到文件中，并将文件名添加到item中。

```python
from urllib.parse import quote

import scrapy
from itemadapter import ItemAdapter

class ScreenshotPipeline:
    """Pipeline that uses Splash to render screenshot of
    every Scrapy item."""

    SPLASH_URL = "http://localhost:8050/render.png?url={}"

    async def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        encoded_item_url = quote(adapter["url"])
        screenshot_url = self.SPLASH_URL.format(encoded_item_url)
        request = scrapy.Request(screenshot_url)
        response = await spider.crawler.engine.download(request, spider)

        if response.status != 200:
            # Error happened, return item.
            return item

        # Save screenshot to file, filename will be hash of url.
        url = adapter["url"]
        url_hash = hashlib.md5(url.encode("utf8")).hexdigest()
        filename = f"{url_hash}.png"
        with open(filename, "wb") as f:
            f.write(response.body)

        # Store filename in item.
        adapter["screenshot_filename"] = filename
        return item
```

### 重复过滤器

一个过滤器，用于查找重复项，并删除那些重复的item。假设我们的item具有唯一的ID，但是我们的Spider返回了具有相同ID的多个item：

```python
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem

class DuplicatesPipeline:

    def __init__(self):
        self.ids_seen = set()

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        if adapter['id'] in self.ids_seen:
            raise DropItem(f"Duplicate item found: {item!r}")
        else:
            self.ids_seen.add(adapter['id'])
            return item
```

## 激活Item Pipeline组件

要激活Item Pipeline组件，您必须将其类添加到 [`ITEM_PIPELINES`](https://docs.scrapy.org/en/latest/topics/settings.html#std-setting-ITEM_PIPELINES) 设置中，如以下示例所示：

```python
ITEM_PIPELINES = {
    'myproject.pipelines.PricePipeline': 300,
    'myproject.pipelines.JsonWriterPipeline': 800,
}
```

您在此设置中分配给类的整数值确定它们运行的顺序：item从值较低的类转到值较高的类。通常将这些数字定义在0-1000范围内。

## 常用的自定义Item pipeline

### 自定义json pipeline

ArticleSpider/pipelines.py

```python
import codecs
import json

class JsonWithEncodingPipeline(object):
    def __init__(self):
        self.file = codecs.open('article.json', 'w', encoding="utf-8")

    def process_item(self, item, spider):
        lines = json.dumps(dict(item), ensure_ascii=False) + "\n"
        self.file.write(lines)
        return item

    def spider_closed(self, spider):
        self.file.close()
```

注册json pipeline

ArticleSpider/settings.py

```bash
...
ITEM_PIPELINES = {
	...
    'ArticleSpider.pipelines.JsonWithEncodingPipeline':2
}
...
```



### 自定义json pipeline

基于scrapy提供的jsonItemExporter

ArticleSpider/pipelines.py

```python
from scrapy.exporters import JsonItemExporter

class JsonExporterPipeline(object):
    """ 调用scrapy提供的jsonItemExporter导出json文件
    """

    def __init__(self):
        self.file = open('hello.json', 'wb')
        self.exporter = JsonItemExporter(self.file, encoding='utf-8', ensure_ascii=False)
        self.exporter.start_exporting()

    def process_item(self, item, spider):
        self.exporter.export_item(item)
        return item

    def close_spider(self, spider):
        self.exporter.finish_exporting()
        self.file.close()
```

注册json pipeline

ArticleSpider/settings.py

```bash
...
ITEM_PIPELINES = {
	...
    'ArticleSpider.pipelines.JsonExporterPipeline':2
}
...
```



### 自定义Mysql Pipeline

ArticleSpider/pipelines.py

```python
import MySQLdb

class MysqlPipeline(object):
    """写入mysql数据库（同步方式）
    """

    def __init__(self):
        self.conn = MySQLdb.connect('127.0.0.1', 'root', '123456aB', 'spider', charset="utf8", use_unicode=True)
        self.cursor = self.conn.cursor()

    def process_item(self, item, spider):
        insert_sql = """insert into imooc(course_name,course_duration,course_chapters) values (%s,%s,%s)"""
        self.cursor.execute(insert_sql, [item['course_name'], item['course_duration'],
                                         json.dumps(item['course_chapters'], ensure_ascii=False)])
        self.conn.commit()
```

注册json pipeline

ArticleSpider/settings.py

```bash
...
ITEM_PIPELINES = {
	...
    'ArticleSpider.pipelines.MysqlPipeline':2
}
...
```

### 自定义Mysql Pipeline

ArticleSpider/pipelines.py

```python
import MySQLdb.cursors
from twisted.enterprise import adbapi

class MysqlTwistedPipeline(object):

    def __init__(self, db_pool):
        self.db_pool = db_pool

    @classmethod
    def from_settings(cls, settings):
        db_params = dict(
            host=settings["MYSQL_HOST"],
            db=settings["MYSQL_DB_NAME"],
            user=settings["MYSQL_USER"],
            password=settings["MYSQL_PASSWORD"],
            charset="utf8",
            cursorclass=MySQLdb.cursors.DictCursor,
            use_unicode=True,
        )
        db_pool = adbapi.ConnectionPool("MySQLdb", **db_params)
        return cls(db_pool)

    def process_item(self, item, spider):
        # 使用twisted将mysql插入转为异步执行
        query = self.db_pool.runInteraction(self.do_insert, item)
        # 异步处理异常
        query.addErrback(self.handle_error, item, spider)

    def do_insert(self, cursor, item):
        insert_sql = """insert into imooc(course_name,course_duration,course_chapters) values (%s,%s,%s)"""
        cursor.execute(insert_sql, [item['course_name'], item['course_duration'],
                                    json.dumps(item['course_chapters'], ensure_ascii=False)])

    def handle_error(self, failure, item, spider):
        # 处理异步插入异常
        print(failure)
```

注册json pipeline

ArticleSpider/settings.py

```bash
...
ITEM_PIPELINES = {
	...
    'ArticleSpider.pipelines.MysqlTwistedPipeline':2
}
...
```

